<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Hierarchical Conditioning of Diffusion Models Using Tree-of-Life for Studying Species Evolution</title>
  <link rel="stylesheet" href="css/main.css" />
  <link rel="icon" href="images/icons/favicon.ico" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+Display:ital,wght@0,400;0,700;1,400;1,700&display=swap" rel="stylesheet">
</head>

<body>
  <main>
    <div style="text-align: center">
      <details>
        <summary>More Research</summary>
        <div class="options">
          <p><a href="https://imageomics.github.io/bioclip/">BioCLIP</a></p>
          <p><a href="https://github.com/Imageomics/INTR">INTR</a></p>
        </div>
      </details>
    </div>
    <h1>Hierarchical Conditioning of Diffusion Models Using Tree-of-Life for Studying Species Evolution</h1>
    <p class="centered">
      <sup>1</sup><a href="https://mridulk97.github.io/">Mridul Khurana</a>,
      <sup>1</sup><a href="https://scholar.google.com/citations?user=pz2Nm8AAAAAJ&hl=en">Arka Daw</a>,
      <sup>1</sup><a href="https://people.cs.vt.edu/marufm/">M. Maruf</a>,
      <sup>1</sup><a href="https://www.uyedalab.com/">Josef Uyeda</a>,
      <sup>2</sup><a href="https://www.faculty.uci.edu/profile/?facultyId=6769">Wasila Dahdul</a>,
      <sup>1</sup><a href="https://scholar.google.com/citations?user=LyaJ880AAAAJ&hl=en">Caleb Charpentier</a>,
      <sup>3</sup><a href="https://scholar.google.com/citations?user=bRO-9m8AAAAJ&hl=en">Yasin Bakış</a>,
      <sup>3</sup><a href="https://scholar.google.com/citations?user=K4typXsAAAAJ&hl=en">Henry L. Bart Jr.</a>,
      <sup>4</sup><a href="https://scholar.google.com/citations?user=ZKnlvcoAAAAJ&hl=en">Paula Mabee</a>,
      <sup>5</sup><a href="https://scholars.duke.edu/person/Hilmar.Lapp">Hilmar Lapp</a>,
      <sup>6</sup><a href="https://renci.org/staff/james-balhoff/">James Balhoff</a>,
      <sup>7</sup><a href="https://sites.google.com/view/wei-lun-harry-chao">Wei-Lun (Harry) Chao</a>,
      <sup>8</sup><a href="https://www.cs.rpi.edu/~stewart/">Charles Stewart</a>,
      <sup>7</sup><a href="https://cse.osu.edu/people/berger-wolf.1">Tanya Berger-Wolf</a>,
      <sup>1</sup><a href="https://people.cs.vt.edu/karpatne/">Anuj Karpatne</a>
    </p>
    <p class="centered">
      <sup>1</sup>Virginia Tech,
      <sup>2</sup>University of California, Irvine,
      <sup>3</sup>Tulane University,
      <sup>4</sup>Battelle,
      <sup>5</sup>Duke University,
      <sup>6</sup>University of North Carolina at Chapel Hill,
      <sup>7</sup>The Ohio State University,
      <sup>8</sup>Rensselaer Polytechnic Institute
    </p>
    <!-- <p class="text-sm centered">
      <sup>*</sup>Sam and Lisa are co-first authors and contributed equally to BioCLIP.
    </p> -->
    <p class="text-sm centered">
      <a href="mailto:mridul@vt.edu">mridul@vt.edu</a>, 
      <a href="mailto:karpatne@vt.edu">karpatne@vt.edu</a>
    </p>

    <p class="centered">
      <!-- <a class="pill-button" href="https://huggingface.co/datasets/imageomics/TreeOfLife-10M">
        <img src="images/icons/huggingface.svg" /> Data
      </a> -->
      <a class="pill-button" href="https://github.com/Imageomics/phylo-diffusion">
        <img src="images/icons/github.svg" /> Code
      </a>
      <a class="pill-button" href="https://huggingface.co/spaces/mridulk/hier-embed">
        <img src="images/icons/huggingface.svg" /> Demo
      </a>
      <!-- <a class="pill-button" href="https://arxiv.org/abs/2311.18803">
        <img src="images/icons/arxiv.svg" /> Paper
      </a> -->
    </p>
    <figure>
      <img srcset="" src="images/phylo_diffusion_architecture.jpg" alt="" loading="lazy">
      <figcaption>
        Figure 1: Overview of Phylo-Diffusion framework. Every species in the tree of life (phylogenetic tree) is encoded to a 
        HIERarchical Embedding (HIER-Embed) comprising of four vectors (one for each phylogenetic level), which is used to 
        condition a latent diffusion model to generate synthetic images of the species. By structuring the embedding space with 
        phylogenetic knowledge, Phylo-Diffusion enables visualization of changes in the evolutionary traits of a species 
        (circled pink) upon perturbing its embedding.
      </figcaption>
    </figure>

    <h2 class="banded">Phylo-Diffusion</h2>

    <p>
      We introduce Graph as a modality to conditional Diffusion Models, Phylo-Diffusion, for understanding in evolutionary traits in biological specimens.
      We leverage this new modality and introduce two experiments for understanding evolutionary traits inspired by gene-knockout experiments:
      <ol>
        <li><b>Trait Masking:</b> where one or more level encodings of species are masked out with noise.</li>
        <li><b>Trait Swapping:</b> which involves the replacement of a specific level encoding of a reference species with that of a sibling node</li>
      </ol>
    </p>
        
    <h3>Demo</h3>
    <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/4.36.1/gradio.js"></script>
    <gradio-app src="https://mridulk-hier-embed.hf.space/"></gradio-app>

    <h2 class="banded" id="evaluation">Experiments</h2>
    <p>
      We evaluate Phylo-Diffusion and show that it achieves high-fidelity at par with the text-to-image and class-conditional diffusion models.
      TODO: Insert table
    </p>

    <h3>Trait Masking</h3>

    <figure>

      <img srcset="" src="images/level3-lepomis.jpg" alt="" loading="lazy">
      <figcaption>
        Figure 2: Probability activations for <i>Lepomis</i> species at level 3 for Trait Masking. 
        The species highlighted in green represent the species belonging to that subtree at the given level
      </figcaption>
    </figure>
    
    <h3>Trait Swapping</h3>

    <figure>

      <img srcset="" src="images/trait_swapping1.jpg" alt="" loading="lazy">
      <figcaption>
        Figure 3: Visualization of changes in traits after swapping information at Level 2 (Node A) 
        for <i>Noturus exilis</i> (left) with its sibling subtree at Node B(right) to generate perturbed species (center). 
        Traits shared with the source species are outlined in green, whereas those shared with the sibling subtree 
        at Node B are outlined in pink.
      </figcaption>
    </figure>
    


    <!-- <p><i>Scroll to see all results.</i></p>
    <table cellpadding="0" cellspacing="0">
      <thead>
        <tr>
          <th rowspan="2" class="sticky border-right">Model</th>
          <th colspan="4" class="no-border-bottom border-right">Animals</th>
          <th colspan="5" class="no-border-bottom border-right">Plants & Fungi</th>
          <th rowspan="2" class="border-right">Rare Species</th>
          <th rowspan="2">Mean</th>
        </tr>
        <tr>
          <th class="no-border-top border-left">Birds 525</th>
          <th class="no-border-top">Plankton</th>
          <th class="no-border-top">Insects</th>
          <th class="no-border-top border-right">Insects 2</th>
          <th class="no-border-top">PlantNet</th>
          <th class="no-border-top">Fungi</th>
          <th class="no-border-top">PlantVillage</th>
          <th class="no-border-top">Med. Leaf</th>
          <th class="no-border-top border-right">PlantDoc</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td class="sticky border-right">CLIP</td>
          <td>49.9</td>
          <td>3.2</td>
          <td>9.1</td>
          <td>9.8</td>
          <td>58.5</td>
          <td>10.2</td>
          <td>5.4</td>
          <td>15.9</td>
          <td>26.1</td>
          <td>26.6</td>
          <td>21.4</td>
        </tr>
        <tr>
          <td class="sticky border-right">OpenCLIP</td>
          <td>54.7</td>
          <td>2.2</td>
          <td>6.5</td>
          <td>9.6</td>
          <td>50.2</td>
          <td>5.7</td>
          <td>8.0</td>
          <td>12.4</td>
          <td>25.8</td>
          <td>31.0</td>
          <td>20.6</td>
        </tr>
        <tr>
          <td class="sticky border-right">BioCLIP</td>
          <td><b>72.1</b></td>
          <td><b>6.1</b></td>
          <td><b>34.8</b></td>
          <td><b>20.4</b></td>
          <td><b>91.4</b></td>
          <td>40.7</td>
          <td><b>24.4</b></td>
          <td><b>38.6</b></td>
          <td><b>28.4</b></td>
          <td><b>37.8</b></td>
          <td><b>39.4</b></td>
        </tr>
        <tr>
          <td class="sticky border-right">iNat21 Only</td>
          <td>56.1</td>
          <td>2.6</td>
          <td>30.7</td>
          <td>11.5</td>
          <td>88.2</td>
          <td><b>43.0</b></td>
          <td>18.4</td>
          <td>25.6</td>
          <td>20.5</td>
          <td>19.4</td>
          <td>31.6</td>
        </tr>
      </tbody>
    </table> -->



    <h2 id="comparision" class="banded">Comparision to PhyloNN</h2>
    <!-- <p>
      Why does BioCLIP work so well?
      We conduct an intrinsic evaluation to understand the representations learned by BioCLIP.
      We visualize BioCLIP and CLIP's representations for the 100K unseen images in the iNat21 validation set, using T-SNE to plot them in two-dimensions, coloring the points based on their class.
      In the figure below, (B) means <b>B</b>ioCLIP and (O) means <b>O</b>penAI's CLIP.
    </p>
    <p>
      At higher ranks like phylum, both CLIP and BioCLIP have good separation, but you can see that BioCLIP's representation is more fine-grained and contains a richer clustering structure.
      At lower ranks, BioCLIP produces far more separable features, while CLIP's features tend to be cluttered and lack a clear structure.
      This shows that BioCLIP has learned a rich feature representation following the hierarchical structure of the taxonomy, which helps explain its strong generalization across the tree of life.
    </p>
    <figure>
      <a href="files/intrinsic.pdf">
        <img srcset="" src="images/intrinsic_small.png" alt="" loading="lazy">
      </a>
      <figcaption>
        We show that <b>(B)</b>ioCLIP's representations are more fine-grained and contain a richer clustering structure than <b>(O)</b>penAI's CLIP.
        Click on the image to see the full resolution PDF, or check out our <a href="https://arxiv.org/abs/2311.18803">paper</a> for more details.
      </figcaption>
    </figure> -->

    <!-- <h2 class="banded" id="dataset">Dataset</h2>
    <p>
      TreeOfLife-10M is the largest and most diverse available dataset of biology images.
      We combine images from three sources, iNaturalist, BIOSCAN-1M, and Encyclopedia of Life (<a href="https://eol.org">EOL</a>, accessed 29 July 2023), to create a dataset of
      10M images, spanning 450K+ species.
      We train BioCLIP on TreeOfLife-10M and release the weights for public use.
    </p> -->

    <!-- <h2>Reference</h2>
    <p>Please cite our paper if you use our code, data, model or results.</p>
    <pre class="reference">@inproceedings{stevens2024bioclip,
      title = {Hier}, 
      author = {},
      booktitle={},
      year = {2024}
    }</pre>
    <p>Also consider citing LDM:</p>
    <pre class="reference">@inproceedings{rombach2022high,
      title={High-resolution image synthesis with latent diffusion models},
      author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
      booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
      pages={10684--10695},
      year={2022}
    }</pre> -->

    <h2 class="banded">Acknowledgements</h2>
    <p>
      This work was supported by the <a href="https://imageomics.org">Imageomics Institute</a>, which is funded by the US National Science Foundation's Harnessing the Data Revolution (HDR) program under <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=2118240">Award #2118240</a> (Imageomics: A New Frontier of Biological Information Powered by Knowledge-Guided Machine Learning). Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.
    </p>
  </main>
  <script>
    const details = document.querySelector("details");
    document.addEventListener("click", function (e) {
      if (!details.contains(e.target)) {
        details.removeAttribute("open");
      }
    });
  </script>
</body>

</html>